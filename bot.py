import os
import asyncio
import logging

from dotenv import load_dotenv
from openai import OpenAI

from aiogram import Bot, Dispatcher, F
from aiogram.enums import ParseMode
from aiogram.types import Message

from flask import Flask
from hypercorn.asyncio import serve
from hypercorn.config import Config

# ---------- ENV ----------
load_dotenv()
TG_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
PORT = int(os.getenv("PORT", "10000"))

if not TG_TOKEN: raise RuntimeError("TELEGRAM_BOT_TOKEN –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
if not OPENAI_KEY: raise RuntimeError("OPENAI_API_KEY –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

os.environ["OPENAI_API_KEY"] = OPENAI_KEY
client = OpenAI()

# ---------- LOGGING ----------
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
log = logging.getLogger("fixfur-bot")

# ---------- Aiogram ----------
bot = Bot(token=TG_TOKEN, parse_mode=ParseMode.HTML)
dp = Dispatcher()

SYSTEM_PROMPT = (
    "–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–≥–æ –º–µ—Ö–æ–≤–æ–≥–æ –∞—Ç–µ–ª—å–µ ¬´FIX FUR by ATARSHCHIKOV¬ª. "
    "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —É–≤–µ—Ä–µ–Ω–Ω–æ –∏ –ø–æ –¥–µ–ª—É. –ü—Ä–µ–¥–ª–∞–≥–∞–π —Ä–µ—à–µ–Ω–∏—è –ø–æ –ø–µ—Ä–µ—à–∏–≤—É, —Ä–µ—Å—Ç–∞–≤—Ä–∞—Ü–∏–∏, —É—Ö–æ–¥—É."
)

# helper: –±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–µ–ª–∏–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –ø–æ 3500 —Å–∏–º–≤–æ–ª–æ–≤
def chunk(text: str, size: int = 3500):
    for i in range(0, len(text), size):
        yield text[i:i+size]

# ---------- /start ----------
@dp.message(F.text == "/start")
async def on_start(message: Message):
    welcome = (
        "–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ <b>FIX FUR by ATARSHCHIKOV</b> üß•\n"
        "–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –æ —Ä–µ—Å—Ç–∞–≤—Ä–∞—Ü–∏–∏, –ø–µ—Ä–µ—à–∏–≤–µ, —Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–ª–∏ —É—Ö–æ–¥–µ –∑–∞ –º–µ—Ö–æ–º ‚Äî –ø–æ–¥—Å–∫–∞–∂—É –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç."
    )
    await message.answer(welcome)

# ---------- –¢–µ–∫—Å—Ç ----------
@dp.message(F.text & ~F.text.startswith("/"))
async def on_text(message: Message):
    user_text = message.text or ""
    try:
        resp = await asyncio.to_thread(
            lambda: client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_text},
                ],
                temperature=0.5,
                max_tokens=700,
            )
        )
        reply = resp.choices[0].message.content.strip()
    except Exception as e:
        reply = f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤—Ä–µ–º–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}"
    for part in chunk(reply):
        await message.answer(part)

# ---------- –§–æ—Ç–æ/–≤–∏–¥–µ–æ (–±–µ—Ä—ë–º –ø–æ–¥–ø–∏—Å—å –∏ –æ—Ç–≤–µ—á–∞–µ–º) ----------
@dp.message(F.photo | F.video | F.document & ~F.document.file_name.endswith(".oga"))
async def on_media(message: Message):
    caption = message.caption or "–ü—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ/–≤–∏–¥–µ–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –º–µ—Ö–æ–≤–æ–≥–æ –∞—Ç–µ–ª—å–µ."
    try:
        resp = await asyncio.to_thread(
            lambda: client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": caption},
                ],
                temperature=0.5,
                max_tokens=600,
            )
        )
        reply = resp.choices[0].message.content.strip()
    except Exception as e:
        reply = f"–ü–æ–ª—É—á–∏–ª —Ñ–∞–π–ª. –ü–æ–∫–∞ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {e}"
    await message.answer(reply)

# ---------- –ì–æ–ª–æ—Å–æ–≤—ã–µ: —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ Whisper ----------
@dp.message(F.voice | (F.document & F.document.mime_type == "audio/ogg"))
async def on_voice(message: Message):
    try:
        file = await bot.get_file(message.voice.file_id if message.voice else message.document.file_id)
        file_url = f"https://api.telegram.org/file/bot{TG_TOKEN}/{file.file_path}"
        # —Å–∫–∞—á–∏–≤–∞–µ–º –≤ –ø–∞–º—è—Ç—å
        import requests, tempfile
        with tempfile.NamedTemporaryFile(suffix=".ogg", delete=False) as tmp:
            r = requests.get(file_url, timeout=60)
            tmp.write(r.content)
            tmp_path = tmp.name
        # –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ Whisper
        with open(tmp_path, "rb") as f:
            tr = client.audio.transcriptions.create(model="whisper-1", file=f)
        user_text = tr.text.strip() if hasattr(tr, "text") else tr["text"].strip()
        # –æ—Ç–≤–µ—á–∞–µ–º –∫–∞–∫ –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
        resp = await asyncio.to_thread(
            lambda: client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_text},
                ],
                temperature=0.5,
                max_tokens=700,
            )
        )
        reply = resp.choices[0].message.content.strip()
        await message.answer(f"–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞: ¬´{user_text}¬ª\n\n{reply}")
    except Exception as e:
        await message.answer(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ: {e}")

# ---------- Flask (healthcheck) ----------
app = Flask(__name__)

@app.get("/")
def health():
    return "ok", 200

async def run_flask():
    cfg = Config()
    cfg.bind = [f"0.0.0.0:{PORT}"]
    await serve(app, cfg)

async def run_aiogram():
    await dp.start_polling(bot)

# ---------- Entry ----------
async def main():
    await asyncio.gather(run_aiogram(), run_flask())

if __name__ == "__main__":
    asyncio.run(main())
